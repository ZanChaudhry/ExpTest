{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9pIp1BfO_fb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import linregress, t, f\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from scipy.optimize import curve_fit\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import warnings\n",
        "import json\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    # Set seed for CPU\n",
        "    torch.manual_seed(seed)\n",
        "    # Set seed for GPU (if available)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multiple GPUs\n",
        "    # Ensure deterministic behavior\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    # Set seed for NumPy\",\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for Python's random module\",\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "Hm1_uic1Ppdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "# Learning rate boundary calculation\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X = scaler_X.fit_transform(X)\n",
        "\n",
        "X = X.T\n",
        "\n",
        "# Calculate Sigma\n",
        "Sigma = (1 / (X.shape[1] - 1)) * X @ X.T\n",
        "\n",
        "# Calculate eigenvalues of Sigma\n",
        "eigenvalues = np.linalg.eigvals(Sigma)\n",
        "\n",
        "# Find the maximum eigenvalue\n",
        "max_eigenvalue = np.max(eigenvalues)\n",
        "\n",
        "#Find the maximum learning rate\n",
        "max_lr = 2 * X.shape[1] / (max_eigenvalue * (X.shape[1] - 1))\n",
        "\n",
        "# Print the maximum learning rate\n",
        "print(f\"Maximum Learning Rate: {max_lr}\")"
      ],
      "metadata": {
        "id": "NvohSXLCPr8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_dim, width, depth):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, width))\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(depth - 1):\n",
        "            layers.append(nn.Linear(width, width))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(width, 1))  # Output layer\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "QJ3NojZ7P2Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_test(loss, t_start):\n",
        "  with warnings.catch_warnings():\n",
        "      # Ignore all warnings in this block\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      def exponential_decay(x, a, b, c):\n",
        "        # Calculate the exponent\n",
        "        exponent = -b * x\n",
        "        return a * exponent + c\n",
        "\n",
        "      x = np.arange(t_start, len(loss)+t_start)\n",
        "      y = np.array(loss)\n",
        "      p_exp, _ = curve_fit(exponential_decay, x, y) # Parameters of exponential decay fitting\n",
        "      lin_slope, lin_intercept, r, p, se = linregress(x, y) # Parameters of linear fitting\n",
        "      ss_exp = np.sum((y - exponential_decay(x, *p_exp))**2) # Residuals for exponential\n",
        "      ss_lin = np.sum((y - (lin_slope*x + lin_intercept))**2) # Residuals for linear\n",
        "      df1 = len(x) - 2 # Degrees of freedom for linear\n",
        "      df2 = len(x) - 3 # Degrees of freedom for exponential\n",
        "      f_stat = (ss_lin - ss_exp) / (df1 - df2) / (ss_exp / df2) # F-test of residuals\n",
        "      p = 1 - f.cdf(f_stat, df1-df2, df2)\n",
        "      return p\n",
        "\n",
        "def linear_test(loss, t_start):\n",
        "  x = np.arange(t_start, len(loss)+t_start)\n",
        "  y = np.array(loss)\n",
        "  slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "  # Calculate the t-statistic\n",
        "  t_stat = slope / std_err\n",
        "  # Degrees of freedom\n",
        "  df = len(x) - 2\n",
        "  # Calculate the one-tailed p-value\n",
        "  p = t.cdf(t_stat, df)\n",
        "  return p"
      ],
      "metadata": {
        "id": "WXysFIXIP6Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_exptest(model, max_lr, alpha=0.05, beta=0.1, epochs=50):\n",
        "    # Load California Housing Data\n",
        "    data = fetch_california_housing()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # Split Data\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "    # Standardize Data\n",
        "    scaler_X = StandardScaler()\n",
        "    X_train = scaler_X.fit_transform(X_train)\n",
        "    X_val = scaler_X.transform(X_val)\n",
        "    X_test = scaler_X.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Check for GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize Model, Loss Function, and Optimizer\n",
        "    input_dim = X_train.shape[1]\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=max_lr)\n",
        "\n",
        "    # Move data to GPU\n",
        "    X_train_tensor = X_train_tensor.to(device)\n",
        "    y_train_tensor = y_train_tensor.to(device)\n",
        "    X_val_tensor = X_val_tensor.to(device)\n",
        "    y_val_tensor = y_val_tensor.to(device)\n",
        "    X_test_tensor = X_test_tensor.to(device)\n",
        "    y_test_tensor = y_test_tensor.to(device)\n",
        "\n",
        "    overall_losses = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    exp_test = False\n",
        "    t_start = 0\n",
        "    window = None\n",
        "    curr_lr = max_lr\n",
        "    initial_loss = None\n",
        "    best_model = None\n",
        "    best_val = float('inf')\n",
        "    count = 0\n",
        "    while True:\n",
        "      if count == epochs:\n",
        "          break\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(X_train_tensor)\n",
        "      train_loss = criterion(outputs, y_train_tensor)\n",
        "      train_losses.append(train_loss.detach().item())\n",
        "      overall_losses.append(train_loss.detach().item())\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if initial_loss is None:\n",
        "        initial_loss = train_loss.detach().item()\n",
        "\n",
        "      if window == None:\n",
        "        window = round(2 * np.sqrt(2) * initial_loss / (max_lr * np.exp(1)))\n",
        "\n",
        "      elif (len(train_losses) == window):\n",
        "        if exp_test == False:\n",
        "          p = exponential_test(train_losses, t_start)\n",
        "          if p < alpha:\n",
        "            exp_test = True\n",
        "            t_start += len(train_losses)\n",
        "            train_losses = []\n",
        "          else:\n",
        "            curr_lr *= beta\n",
        "            model = FeedForwardNN(input_dim, 32, 2).to(device)\n",
        "            optimizer = optim.SGD(model.parameters(), lr=curr_lr)\n",
        "            window = round(2 * np.sqrt(2) * initial_loss / (curr_lr * np.exp(1)))\n",
        "            train_losses = []\n",
        "\n",
        "        else:\n",
        "            p = linear_test(train_losses, t_start)\n",
        "            if p < alpha:\n",
        "              t_start += len(train_losses)\n",
        "              train_losses = []\n",
        "            else:\n",
        "              t_start += len(train_losses)\n",
        "              curr_lr *= beta\n",
        "              optimizer = optim.SGD(model.parameters(), lr=curr_lr)\n",
        "              window = round(2 * np.sqrt(2) * initial_loss / (curr_lr * np.exp(1)))\n",
        "              train_losses = []\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad(): # Validation loss\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "      if (val_loss.item() < best_val):\n",
        "        best_model = model.state_dict()\n",
        "        best_val = val_loss.item()\n",
        "\n",
        "      val_losses.append(val_loss.detach().item())\n",
        "      count += 1\n",
        "    model.load_state_dict(best_model)\n",
        "    # Evaluate on Test Set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test_tensor)\n",
        "        test_loss = criterion(test_outputs, y_test_tensor)\n",
        "\n",
        "    print(f'Test Loss: {test_loss.detach().item()}')\n",
        "\n",
        "    return overall_losses, val_losses, test_loss.detach().item()"
      ],
      "metadata": {
        "id": "9bjlCvgiBXJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, epochs=50):\n",
        "    # Load California Housing Data\n",
        "    data = fetch_california_housing()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # Split Data\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "    # Standardize Data\n",
        "    scaler_X = StandardScaler()\n",
        "    X_train = scaler_X.fit_transform(X_train)\n",
        "    X_val = scaler_X.transform(X_val)\n",
        "    X_test = scaler_X.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Check for GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize Model, Loss Function, and Optimizer\n",
        "    input_dim = X_train.shape[1]\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Move data to GPU\n",
        "    X_train_tensor = X_train_tensor.to(device)\n",
        "    y_train_tensor = y_train_tensor.to(device)\n",
        "    X_val_tensor = X_val_tensor.to(device)\n",
        "    y_val_tensor = y_val_tensor.to(device)\n",
        "    X_test_tensor = X_test_tensor.to(device)\n",
        "    y_test_tensor = y_test_tensor.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_model = None\n",
        "    best_val = float('inf')\n",
        "    count = 0\n",
        "\n",
        "    while True:\n",
        "      if count == epochs:\n",
        "          break\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(X_train_tensor)\n",
        "      train_loss = criterion(outputs, y_train_tensor)\n",
        "      train_losses.append(train_loss.detach().item())\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        val_outputs = model(X_val_tensor)\n",
        "        val_loss = criterion(val_outputs, y_val_tensor)\n",
        "      if val_loss.item() < best_val:\n",
        "       best_model = model.state_dict()\n",
        "       best_val = val_loss.item()\n",
        "\n",
        "      val_losses.append(val_loss.detach().item())\n",
        "      count += 1\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    # Evaluate on Test Set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test_tensor)\n",
        "        test_loss = criterion(test_outputs, y_test_tensor)\n",
        "\n",
        "    print(f'Test Loss: {test_loss.detach().item()}')\n",
        "\n",
        "    return train_losses, val_losses, test_loss.detach().item()"
      ],
      "metadata": {
        "id": "a7kpLb9zCKJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dict_to_file(dictionary, file_path):\n",
        "    \"\"\"\n",
        "    Saves a dictionary to a text file.\n",
        "\n",
        "    Parameters:\n",
        "    - dictionary (dict): The dictionary to be saved.\n",
        "    - file_path (str): The path to the file where the dictionary will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'w') as file:\n",
        "            json.dump(dictionary, file, indent=4)\n",
        "        print(f\"Dictionary successfully saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving the dictionary: {e}\")"
      ],
      "metadata": {
        "id": "TRpBP3_xCqal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training block for ExpTest\n",
        "\n",
        "epochs = 10000\n",
        "num_trials = 5\n",
        "\n",
        "train_losses_dict = {}\n",
        "val_losses_dict = {}\n",
        "test_losses_dict = {}\n",
        "\n",
        "for trial in range(num_trials):\n",
        "  set_seed(trial)\n",
        "\n",
        "  model = FeedForwardNN(X.shape[0], 32, 2).to(device)\n",
        "  # Train with custom SGD optimizer\n",
        "  train_losses, val_losses, test_loss = train_expTest(model, max_lr, alpha=0.05, beta=0.33, epochs=epochs)\n",
        "\n",
        "  if train_losses_dict.get(trial) is None:\n",
        "    train_losses_dict[trial] = [train_losses]\n",
        "  else:\n",
        "    train_losses_dict[trial].append(train_losses)\n",
        "\n",
        "  if val_losses_dict.get(trial) is None:\n",
        "    val_losses_dict[trial] = [val_losses]\n",
        "  else:\n",
        "    val_losses_dict[trial].append(val_losses)\n",
        "\n",
        "  if test_losses_dict.get(trial) is None:\n",
        "    test_losses_dict[trial] = [test_loss]\n",
        "  else:\n",
        "    test_losses_dict[trial].append(test_loss)\n",
        "\n",
        "  print(f\"Test Loss (ExpTest): {test_loss}\")\n",
        "  plt.plot(train_losses)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-6xDKNixCTez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"train_losses_cali_algo.txt\"\n",
        "save_dict_to_file(train_losses_dict, filename)\n",
        "\n",
        "filename = \"val_losses_cali_algo.txt\"\n",
        "save_dict_to_file(val_losses_dict, filename)\n",
        "\n",
        "filename = \"test_losses_cali_algo.txt\"\n",
        "save_dict_to_file(test_losses_dict, filename)"
      ],
      "metadata": {
        "id": "1sI5wM1gCrfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "for trial, loss in test_losses_dict.items():\n",
        "    test_losses.append(loss[0])\n",
        "\n",
        "print(\"Test Loss ExpTest:\", round(np.mean(test_losses), 4), \"+/-\", round(np.std(test_losses), 4))"
      ],
      "metadata": {
        "id": "NRyolv11Cueb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training block for other optimizers - Adam shown as example.\n",
        "# Just change the optimizer and factor variable.\n",
        "\n",
        "epochs = 10000\n",
        "num_trials = 5\n",
        "factor = 1 # Change as needed\n",
        "initial_lr = factor * max_lr\n",
        "\n",
        "train_losses_dict = {}\n",
        "val_losses_dict = {}\n",
        "test_losses_dict = {}\n",
        "\n",
        "for trial in range(num_trials):\n",
        "  set_seed(trial)\n",
        "\n",
        "  model = FeedForwardNN(X.shape[0], 32, 2).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=initial_lr) # Change as needed\n",
        "  train_losses, val_losses, test_loss = train_model(model, epochs=epochs)\n",
        "\n",
        "  if train_losses_dict.get(trial) is None:\n",
        "    train_losses_dict[trial] = [train_losses]\n",
        "  else:\n",
        "    train_losses_dict[trial].append(train_losses)\n",
        "\n",
        "  if val_losses_dict.get(trial) is None:\n",
        "    val_losses_dict[trial] = [val_losses]\n",
        "  else:\n",
        "    val_losses_dict[trial].append(val_losses)\n",
        "\n",
        "  if test_losses_dict.get(trial) is None:\n",
        "    test_losses_dict[trial] = [test_loss]\n",
        "  else:\n",
        "    test_losses_dict[trial].append(test_loss)\n",
        "\n",
        "  print(f\"Test Loss (Adam): {test_loss}\") # Change as needed\n",
        "  plt.plot(train_losses)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jFlVpcNkC3ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change filenames as needed\n",
        "\n",
        "filename = f\"train_losses_cali_Adam_{factor}x.txt\"\n",
        "save_dict_to_file(train_losses_dict, filename)\n",
        "\n",
        "filename = f\"val_losses_cali_Adam_{factor}x.txt\"\n",
        "save_dict_to_file(val_losses_dict, filename)\n",
        "\n",
        "filename = f\"test_losses_cali_Adam_{factor}x.txt\"\n",
        "save_dict_to_file(test_losses_dict, filename)"
      ],
      "metadata": {
        "id": "5333Ng3dDXJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "for trial, loss in test_losses_dict.items():\n",
        "    test_losses.append(loss[0])\n",
        "# Change optimizer name as needed\n",
        "print(\"Test Loss Adam:\", round(np.mean(test_losses), 4), \"+/-\", round(np.std(test_losses), 4))"
      ],
      "metadata": {
        "id": "yqd5JKrODm3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}