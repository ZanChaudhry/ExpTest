{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKHTBDmJHYyo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import linregress, t, f\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from scipy.optimize import curve_fit\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import warnings\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    # Set seed for CPU\n",
        "    torch.manual_seed(seed)\n",
        "    # Set seed for GPU (if available)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multiple GPUs\n",
        "    # Ensure deterministic behavior\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    # Set seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for Python's random module\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "WB37GUnFHzuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Load the training dataset to calculate mean and std\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Extract data and labels\n",
        "train_data = train_dataset.data.numpy()\n",
        "train_labels = train_dataset.targets.numpy()\n",
        "\n",
        "# Reshape and standardize the data\n",
        "num_samples = train_data.shape[0]\n",
        "train_data = train_data.reshape(num_samples, -1)  # Flatten the images\n",
        "\n",
        "# Standardize Data\n",
        "scaler_X = StandardScaler()\n",
        "train_data = scaler_X.fit_transform(train_data)\n",
        "\n",
        "# Calculate Sigma\n",
        "train_data = train_data.T\n",
        "Sigma = (1 / (train_data.shape[1] - 1)) * train_data @ train_data.T\n",
        "\n",
        "# Calculate eigenvalues of Sigma\n",
        "eigenvalues = np.linalg.eigvals(Sigma)\n",
        "\n",
        "# Find the maximum eigenvalue\n",
        "max_eigenvalue = np.abs(np.max(eigenvalues))\n",
        "\n",
        "# Find the maximum learning rate\n",
        "max_lr = 2 * train_data.shape[1] / (max_eigenvalue * (train_data.shape[1] - 1))\n",
        "\n",
        "# Print the maximum learning rate\n",
        "print(f\"Maximum Learning Rate: {max_lr}\")\n",
        "\n",
        "# Calculate mean and std\n",
        "mean = 0.\n",
        "std = 0.\n",
        "for images, _ in train_loader:\n",
        "    batch_mean = images.mean()\n",
        "    batch_std = images.std()\n",
        "    mean += batch_mean\n",
        "    std += batch_std\n",
        "\n",
        "mean /= len(train_loader)\n",
        "std /= len(train_loader)\n",
        "\n",
        "print(f'Mean: {mean}, Std: {std}')\n",
        "\n",
        "# Define the transform with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean.item()], std=[std.item()]),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the images\n",
        "])\n",
        "\n",
        "# Load the dataset with normalization\n",
        "batch_size = 32\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "VywerTyNH7Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 10)  # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "QJPecqCiIR7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradient_magnitude(model):\n",
        "    # Calculate the magnitude of gradients for all parameters\n",
        "    magnitude = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            magnitude += param.grad.norm().item() ** 2\n",
        "    return magnitude ** 0.5"
      ],
      "metadata": {
        "id": "eHGDYYd1IUu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_test(loss, t_start):\n",
        "  with warnings.catch_warnings():\n",
        "      # Ignore all warnings in this block\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      def exponential_decay(x, a, b, c):\n",
        "        # Calculate the exponent\n",
        "        exponent = -b * x\n",
        "        return a * exponent + c\n",
        "\n",
        "      x = np.arange(t_start, len(loss)+t_start)\n",
        "      y = np.array(loss)\n",
        "      p_exp, _ = curve_fit(exponential_decay, x, y) # Parameters of exponential decay fitting\n",
        "      lin_slope, lin_intercept, r, p, se = linregress(x, y) # Parameters of linear fitting\n",
        "      ss_exp = np.sum((y - exponential_decay(x, *p_exp))**2) # Residuals for exponential\n",
        "      ss_lin = np.sum((y - (lin_slope*x + lin_intercept))**2) # Residuals for linear\n",
        "      df1 = len(x) - 2 # Degrees of freedom for linear\n",
        "      df2 = len(x) - 3 # Degrees of freedom for exponential\n",
        "      f_stat = (ss_lin - ss_exp) / (df1 - df2) / (ss_exp / df2) # F-test of residuals\n",
        "      p = 1 - f.cdf(f_stat, df1-df2, df2)\n",
        "      return p\n",
        "\n",
        "def linear_test(loss, t_start):\n",
        "  x = np.arange(t_start, len(loss)+t_start)\n",
        "  y = np.array(loss)\n",
        "  slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "  # Calculate the t-statistic\n",
        "  t_stat = slope / std_err\n",
        "  # Degrees of freedom\n",
        "  df = len(x) - 2\n",
        "  # Calculate the one-tailed p-value\n",
        "  p = t.cdf(t_stat, df)\n",
        "  return p"
      ],
      "metadata": {
        "id": "no7Q00W8IY3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_exptest(model, max_lr, alpha=0.05, beta=0.33, epochs=10):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=max_lr)\n",
        "\n",
        "  overall_losses = [] # Losses for entire training\n",
        "  train_losses = [] # Losses for current fitting window\n",
        "  exp_test = False # No exponential decay has been detected to begin with\n",
        "  resultant_vector = None # Vector sum of gradients\n",
        "  mags_sum = 0 # Sum of magnitudes of gradients\n",
        "  t_start = 0 # Starting time point of fitting\n",
        "  window = None # Window size of fitting calculations\n",
        "  full_window = None # Full window (incorporating correction factor, if True) of fitting calculations\n",
        "  curr_lr = max_lr # Current learning rate\n",
        "  initial_loss = None # Initial loss value used in window size calculation\n",
        "  correct = True # Window correction is enabled\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      for images, labels in train_loader:\n",
        "          # Training loop\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images.to(device))\n",
        "          loss = criterion(outputs, labels.to(device))\n",
        "          train_losses.append(loss.detach().item())\n",
        "          overall_losses.append(loss.detach().item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if initial_loss is None:\n",
        "            initial_loss = loss.item()\n",
        "          # Store gradients\n",
        "          if window == None:\n",
        "            window = round(2 * np.sqrt(2) * initial_loss / (max_lr * np.exp(1)))\n",
        "          elif (len(train_losses) >=1) and (len(train_losses) < window) and (correct):\n",
        "            curr_gradients = [p.grad for p in model.parameters() if p.grad is not None]\n",
        "            curr_magnitude = calculate_gradient_magnitude(model)\n",
        "            mags_sum += curr_magnitude\n",
        "            vec = torch.cat([p.clone().detach().flatten() for p in curr_gradients if p is not None])\n",
        "            if resultant_vector is None:\n",
        "              resultant_vector = vec\n",
        "            else:\n",
        "              resultant_vector += vec\n",
        "          elif (len(train_losses) == window) and (correct):\n",
        "            correction = mags_sum / torch.norm(resultant_vector).item()\n",
        "            full_window = round(window * correction)\n",
        "          # Perform exponential and linear testing\n",
        "          elif (len(train_losses) == full_window):\n",
        "            if exp_test == False:\n",
        "              p = exponential_test(train_losses, t_start)\n",
        "              if p < alpha:\n",
        "                exp_test = True # Exponential decay detected, turn off future detection\n",
        "                t_start += len(train_losses)\n",
        "                train_losses = [] # Reset losses over current fitting window\n",
        "              else:\n",
        "                curr_lr *= beta # Adjust learning rate\n",
        "                optimizer = optim.SGD(model.parameters(), lr=curr_lr)\n",
        "                window = round(2 * np.sqrt(2) * initial_loss / (curr_lr * np.exp(1))) # Recalculate window size\n",
        "                train_losses = [] # Reset losses over current fitting window\n",
        "                # Reset window correction parameters\n",
        "                mags_sum = 0\n",
        "                resultant_vector = None\n",
        "            else:\n",
        "                p = linear_test(train_losses, t_start)\n",
        "                if p < alpha:\n",
        "                  t_start += len(train_losses)\n",
        "                  train_losses = [] # Reset losses over current fitting window\n",
        "                else:\n",
        "                  t_start += len(train_losses)\n",
        "                  curr_lr *= beta # Adjust learning rate\n",
        "                  optimizer = optim.SGD(model.parameters(), lr=curr_lr)\n",
        "                  window = round(2 * np.sqrt(2) * initial_loss / (curr_lr * np.exp(1))) # Recalculate window size\n",
        "                  train_losses = [] # Reset losses over curent fitting window\n",
        "                  # Reset window correction parameters\n",
        "                  mags_sum = 0\n",
        "                  resultant_vector = None\n",
        "\n",
        "  return overall_losses"
      ],
      "metadata": {
        "id": "qm-G7QonIdG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, epochs=10):\n",
        "    # Standard training loop\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    full_train_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images.to(device))\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            full_train_losses.append(loss.item())\n",
        "    return full_train_losses"
      ],
      "metadata": {
        "id": "_eljoNn7KJ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_test_accuracy(model, test_dataloader):\n",
        "    # Standard test accuracy calculation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            outputs = model(images.to(device))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.detach().cpu() == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "UKCZGcoZKOoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average(data, window_size):\n",
        "    data = np.array(data).astype(float)\n",
        "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')"
      ],
      "metadata": {
        "id": "vm4J0O5TKaJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_nested_lists_to_file(nested_lists, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for outer_list in nested_lists:\n",
        "          if isinstance(outer_list[0], list):\n",
        "            for inner_list in outer_list:\n",
        "                # Convert each element to string and join them\n",
        "                file.write(\", \".join(str(item) for item in inner_list) + \"\\n\")\n",
        "            file.write(\"\\n\")  # Add a newline for separation between outer lists\n",
        "          else:\n",
        "            file.write(\", \".join(str(item) for item in outer_list) + \"\\n\")"
      ],
      "metadata": {
        "id": "BSFGs_j2KdWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "factor = 1 # Change depending on experimental condition\n",
        "initial_lr = factor * max_lr\n",
        "num_trials = 5\n",
        "\n",
        "train_losses_adam_list = []\n",
        "test_accuracy_adam_list = []\n",
        "\n",
        "train_losses_exptest_list = []\n",
        "test_accuracy_exptest_list = []\n",
        "\n",
        "train_losses_sgd_momentum_list = []\n",
        "test_accuracy_sgd_momentum_list = []\n",
        "\n",
        "train_losses_sgd_list = []\n",
        "test_accuracy_sgd_list = []\n",
        "\n",
        "train_losses_rmsprop_list = []\n",
        "test_accuracy_rmsprop_list = []\n",
        "\n",
        "train_losses_adadelta_list = []\n",
        "test_accuracy_adadelta_list = []\n",
        "\n",
        "for trial in range(num_trials):\n",
        "  set_seed(trial)\n",
        "\n",
        "  # Train with Adam\n",
        "  model = LogisticRegression(input_dim)\n",
        "  adam_optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "  train_losses_adam = train_model(model, adam_optimizer, epochs=epochs)\n",
        "  train_losses_adam_list.append(train_losses_adam)\n",
        "  test_accuracy_adam = calculate_test_accuracy(model, test_loader)\n",
        "  test_accuracy_adam_list.append(test_accuracy_adam)\n",
        "  print(f\"Test Accuracy (Adam): {test_accuracy_adam}\")\n",
        "  plt.plot(moving_average(train_losses_adam, round(len(train_dataset)/batch_size)))\n",
        "  plt.show()\n",
        "\n",
        "  # Train with SGD w/ Momentum\n",
        "  model = LogisticRegression(input_dim)\n",
        "  sgd_momentum_optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9)\n",
        "  train_losses_sgd_momentum = train_model(model, sgd_momentum_optimizer, epochs=epochs)\n",
        "  train_losses_sgd_momentum_list.append(train_losses_sgd_momentum)\n",
        "  test_accuracy_sgd_momentum = calculate_test_accuracy(model, test_loader)\n",
        "  test_accuracy_sgd_momentum_list.append(test_accuracy_sgd_momentum)\n",
        "  print(f\"Test Accuracy (SGD with Momentum): {test_accuracy_sgd_momentum}\")\n",
        "  plt.plot(moving_average(train_losses_sgd_momentum, round(len(train_dataset)/batch_size)))\n",
        "  plt.show()\n",
        "\n",
        "  # Train with SGD\n",
        "  model = LogisticRegression(input_dim)\n",
        "  sgd_optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
        "  train_losses_sgd = train_model(model, sgd_optimizer, epochs=epochs)\n",
        "  train_losses_sgd_list.append(train_losses_sgd)\n",
        "  test_accuracy_sgd = calculate_test_accuracy(model, test_loader)\n",
        "  test_accuracy_sgd_list.append(test_accuracy_sgd)\n",
        "  print(f\"Test Accuracy (SGD): {test_accuracy_sgd}\")\n",
        "  plt.plot(moving_average(train_losses_sgd, round(len(train_dataset)/batch_size)))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Train with RMSprop\n",
        "  model = LogisticRegression(input_dim)\n",
        "  rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=initial_lr)\n",
        "  train_losses_rmsprop = train_model(model, rmsprop_optimizer, epochs=epochs)\n",
        "  train_losses_rmsprop_list.append(train_losses_rmsprop)\n",
        "  test_accuracy_rmsprop = calculate_test_accuracy(model, test_loader)\n",
        "  test_accuracy_rmsprop_list.append(test_accuracy_rmsprop)\n",
        "  print(f\"Test Accuracy (RMSprop): {test_accuracy_rmsprop}\")\n",
        "  plt.plot(moving_average(train_losses_rmsprop, round(len(train_dataset)/batch_size)))\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "eiP8kd5eKhZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f\"train_losses_logress_{factor}x.txt\"\n",
        "# Keep order consistent for plotting\n",
        "variable_names = [train_losses_adam_list, train_losses_sgd_momentum_list, train_losses_sgd_list, train_losses_rmsprop_list]\n",
        "write_nested_lists_to_file(variable_names, filename)\n",
        "\n",
        "filename = f\"test_losses_logress_{factor}x.txt\"\n",
        "# Keep order consistent for plotting\n",
        "variable_names = [test_accuracy_adam_list, test_accuracy_sgd_momentum_list, test_accuracy_sgd_list, test_accuracy_rmsprop_list]\n",
        "write_nested_lists_to_file(variable_names, filename)"
      ],
      "metadata": {
        "id": "ld6jM9NFbtwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run once\n",
        "\n",
        "train_losses_exptest_list = []\n",
        "test_accuracy_exptest_list = []\n",
        "\n",
        "train_losses_adadelta_list = []\n",
        "test_accuracy_adadelta_list = []\n",
        "\n",
        "for trial in range(num_trials):\n",
        "  set_seed(trial)\n",
        "\n",
        "  # Initialize the model\n",
        "  input_dim = 28 * 28\n",
        "  model = LogisticRegression(input_dim)\n",
        "\n",
        "  # Train with ExpTest optimizer\n",
        "  train_losses_exptest = train_exptest(model, max_lr, epochs=epochs)\n",
        "  train_losses_exptest_list.append(train_losses_exptest)\n",
        "  test_accuracy_exptest = calculate_test_accuracy(model, test_loader)\n",
        "  test_accuracy_exptest_list.append(test_accuracy_exptest)\n",
        "  print(f\"Test Accuracy (ExpTest): {test_accuracy_exptest}\")\n",
        "  plt.plot(moving_average(train_losses_exptest, round(len(train_dataset)/batch_size)))\n",
        "  plt.show()\n",
        "\n",
        "  # Train with Adadelta\n",
        "  model = LogisticRegression(input_dim)\n",
        "  adadelta_optimizer = optim.Adadelta(model.parameters())\n",
        "  train_losses_adadelta = train_model(model, adadelta_optimizer, epochs=epochs)\n",
        "  train_losses_adadelta_list.append(train_losses_adadelta)\n",
        "  test_accuracy_adadelta = calculate_test_accuracy(model, test_loader)\n",
        "  test_accuracy_adadelta_list.append(test_accuracy_adadelta)\n",
        "  print(f\"Test Accuracy (Adadelta): {test_accuracy_adadelta}\")\n",
        "  plt.plot(moving_average(train_losses_adadelta, round(len(train_dataset)/batch_size)))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Zx7vlJXAbikb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"train_losses_logress_algo.txt\"\n",
        "# Keep order consistent for plotting\n",
        "variable_names = [train_losses_exptest_list]\n",
        "write_nested_lists_to_file(variable_names, filename)\n",
        "\n",
        "filename = \"test_losses_logress_algo.txt\"\n",
        "# Keep order consistent for plotting\n",
        "variable_names = [test_accuracy_exptest_list]\n",
        "write_nested_lists_to_file(variable_names, filename)"
      ],
      "metadata": {
        "id": "ScQKGGB8cQO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"train_losses_logress_adadelta.txt\"\n",
        "# Keep order consistent for plotting\n",
        "variable_names = [train_losses_adadelta_list]\n",
        "write_nested_lists_to_file(variable_names, filename)\n",
        "\n",
        "filename = \"test_losses_logress_adadelta.txt\"\n",
        "# Keep order consistent for plotting\n",
        "variable_names = [test_accuracy_adadelta_list]\n",
        "write_nested_lists_to_file(variable_names, filename)"
      ],
      "metadata": {
        "id": "xPcMLr1ycrFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean +/- STD of Test Accuracies (Adam):\", round(100*np.mean(test_accuracy_adam_list), 2), \"+/-\", round(100*np.std(test_accuracy_adam_list), 4))\n",
        "print(\"Mean +/- STD of Test Accuracies (ExpTest):\", round(100*np.mean(test_accuracy_exptest_list), 2), \"+/-\", round(100*np.std(test_accuracy_exptest_list), 4))\n",
        "print(\"Mean +/- STD of Test Accuracies (SGD with Momentum):\", round(100*np.mean(test_accuracy_sgd_momentum_list), 2), \"+/-\", round(100*np.std(test_accuracy_sgd_momentum_list), 4))\n",
        "print(\"Mean +/- STD of Test Accuracies (SGD):\", round(100*np.mean(test_accuracy_sgd_list), 2), \"+/-\", round(100*np.std(test_accuracy_sgd_list), 4))\n",
        "print(\"Mean +/- STD of Test Accuracies (RMSprop):\", round(100*np.mean(test_accuracy_rmsprop_list), 2), \"+/-\", round(100*np.std(test_accuracy_rmsprop_list), 4))\n",
        "print(\"Mean +/- STD of Test Accuracies (Adadelta):\", round(100*np.mean(test_accuracy_adadelta_list), 2), \"+/-\", round(100*np.std(test_accuracy_adadelta_list), 4))"
      ],
      "metadata": {
        "id": "Pn1NOaRdL-Q1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
