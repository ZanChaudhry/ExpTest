{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9pIp1BfO_fb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import linregress, t, f\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from scipy.optimize import curve_fit\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import warnings\n",
        "import json\n",
        "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    # Set seed for CPU\n",
        "    torch.manual_seed(seed)\n",
        "    # Set seed for GPU (if available)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multiple GPUs\n",
        "    # Ensure deterministic behavior\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "    # Set seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for Python's random module\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "Hm1_uic1Ppdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# Load the training dataset to calculate mean and std\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Extract data and labels\n",
        "train_data = train_dataset.data.numpy()\n",
        "train_labels = train_dataset.targets.numpy()\n",
        "\n",
        "# Reshape and standardize the data\n",
        "num_samples = train_data.shape[0]\n",
        "train_data = train_data.reshape(num_samples, -1)  # Flatten the images\n",
        "\n",
        "# Standardize Data\n",
        "scaler_X = StandardScaler()\n",
        "train_data = scaler_X.fit_transform(train_data)\n",
        "\n",
        "# Calculate Sigma\n",
        "train_data = train_data.T\n",
        "Sigma = (1 / (train_data.shape[1] - 1)) * train_data @ train_data.T\n",
        "\n",
        "# Calculate eigenvalues of Sigma\n",
        "eigenvalues = np.linalg.eigvals(Sigma)\n",
        "\n",
        "# Find the maximum eigenvalue\n",
        "max_eigenvalue = np.abs(np.max(eigenvalues))\n",
        "\n",
        "# Find the maximum learning rate\n",
        "max_lr = 2 * train_data.shape[1] / (max_eigenvalue * (train_data.shape[1] - 1))\n",
        "\n",
        "# Print the maximum learning rate\n",
        "print(f\"Maximum Learning Rate: {max_lr}\")\n",
        "\n",
        "# Calculate mean and std\n",
        "mean = 0.\n",
        "std = 0.\n",
        "for images, _ in train_loader:\n",
        "    batch_mean = images.mean()\n",
        "    batch_std = images.std()\n",
        "    mean += batch_mean\n",
        "    std += batch_std\n",
        "\n",
        "mean /= len(train_loader)\n",
        "std /= len(train_loader)\n",
        "\n",
        "print(f'Mean: {mean}, Std: {std}')\n",
        "\n",
        "# Define the transform with normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean.item()], std=[std.item()]),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the images\n",
        "])\n",
        "\n",
        "# Load the dataset with normalization\n",
        "batch_size = 32\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "NvohSXLCPr8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 10)  # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "QJ3NojZ7P2Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gradient_magnitude(model):\n",
        "    # Calculate the magnitude of gradients for all parameters\n",
        "    magnitude = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            magnitude += param.grad.norm().item() ** 2\n",
        "    return magnitude ** 0.5"
      ],
      "metadata": {
        "id": "BRsVdUDaP4J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_test(loss, t_start):\n",
        "  with warnings.catch_warnings():\n",
        "      # Ignore all warnings in this block\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      def exponential_decay(x, a, b, c):\n",
        "        # Calculate the exponent\n",
        "        exponent = -b * x\n",
        "        return a * exponent + c\n",
        "\n",
        "      x = np.arange(t_start, len(loss)+t_start)\n",
        "      y = np.array(loss)\n",
        "      p_exp, _ = curve_fit(exponential_decay, x, y) # Parameters of exponential decay fitting\n",
        "      lin_slope, lin_intercept, r, p, se = linregress(x, y) # Parameters of linear fitting\n",
        "      ss_exp = np.sum((y - exponential_decay(x, *p_exp))**2) # Residuals for exponential\n",
        "      ss_lin = np.sum((y - (lin_slope*x + lin_intercept))**2) # Residuals for linear\n",
        "      df1 = len(x) - 2 # Degrees of freedom for linear\n",
        "      df2 = len(x) - 3 # Degrees of freedom for exponential\n",
        "      f_stat = (ss_lin - ss_exp) / (df1 - df2) / (ss_exp / df2) # F-test of residuals\n",
        "      p = 1 - f.cdf(f_stat, df1-df2, df2)\n",
        "      return p\n",
        "\n",
        "def linear_test(loss, t_start):\n",
        "  x = np.arange(t_start, len(loss)+t_start)\n",
        "  y = np.array(loss)\n",
        "  slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "  # Calculate the t-statistic\n",
        "  t_stat = slope / std_err\n",
        "  # Degrees of freedom\n",
        "  df = len(x) - 2\n",
        "  # Calculate the one-tailed p-value\n",
        "  p = t.cdf(t_stat, df)\n",
        "  return p"
      ],
      "metadata": {
        "id": "WXysFIXIP6Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_exptest(model, max_lr, alpha=0.05, beta=0.33, epochs=10):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=max_lr)\n",
        "\n",
        "  overall_losses = [] # Losses for entire training\n",
        "  train_losses = [] # Losses for current fitting window\n",
        "  exp_test = False # No exponential decay has been detected to begin with\n",
        "  resultant_vector = None # Vector sum of gradients\n",
        "  mags_sum = 0 # Sum of magnitudes of gradients\n",
        "  t_start = 0 # Starting time point of fitting\n",
        "  window = None # Window size of fitting calculations\n",
        "  full_window = None # Full window (incorporating correction factor, if True) of fitting calculations\n",
        "  curr_lr = max_lr # Current learning rate\n",
        "  initial_loss = None # Initial loss value used in window size calculation\n",
        "  correct = True # Window correction is enabled\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      for images, labels in train_loader:\n",
        "          # Training loop\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images.to(device))\n",
        "          loss = criterion(outputs, labels.to(device))\n",
        "          train_losses.append(loss.detach().item())\n",
        "          overall_losses.append(loss.detach().item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if initial_loss is None:\n",
        "            initial_loss = loss.item()\n",
        "          # Store gradients\n",
        "          if window == None:\n",
        "            window = round(2 * np.sqrt(2) * initial_loss / (max_lr * np.exp(1)))\n",
        "          elif (len(train_losses) >=1) and (len(train_losses) < window) and (correct):\n",
        "            curr_gradients = [p.grad for p in model.parameters() if p.grad is not None]\n",
        "            curr_magnitude = calculate_gradient_magnitude(model)\n",
        "            mags_sum += curr_magnitude\n",
        "            vec = torch.cat([p.clone().detach().flatten() for p in curr_gradients if p is not None])\n",
        "            if resultant_vector is None:\n",
        "              resultant_vector = vec\n",
        "            else:\n",
        "              resultant_vector += vec\n",
        "          elif (len(train_losses) == window) and (correct):\n",
        "            correction = mags_sum / torch.norm(resultant_vector).item()\n",
        "            full_window = round(window * correction)\n",
        "          # Perform exponential and linear testing\n",
        "          elif (len(train_losses) == full_window):\n",
        "            if exp_test == False:\n",
        "              p = exponential_test(train_losses, t_start)\n",
        "              if p < alpha:\n",
        "                exp_test = True # Exponential decay detected, turn off future detection\n",
        "                t_start += len(train_losses)\n",
        "                train_losses = [] # Reset losses over current fitting window\n",
        "              else:\n",
        "                curr_lr *= beta # Adjust learning rate\n",
        "                optimizer = optim.SGD(model.parameters(), lr=curr_lr)\n",
        "                window = round(2 * np.sqrt(2) * initial_loss / (curr_lr * np.exp(1))) # Recalculate window size\n",
        "                train_losses = [] # Reset losses over current fitting window\n",
        "                # Reset window correction parameters\n",
        "                mags_sum = 0\n",
        "                resultant_vector = None\n",
        "            else:\n",
        "                p = linear_test(train_losses, t_start)\n",
        "                if p < alpha:\n",
        "                  t_start += len(train_losses)\n",
        "                  train_losses = [] # Reset losses over current fitting window\n",
        "                else:\n",
        "                  t_start += len(train_losses)\n",
        "                  curr_lr *= beta # Adjust learning rate\n",
        "                  optimizer = optim.SGD(model.parameters(), lr=curr_lr)\n",
        "                  window = round(2 * np.sqrt(2) * initial_loss / (curr_lr * np.exp(1))) # Recalculate window size\n",
        "                  train_losses = [] # Reset losses over curent fitting window\n",
        "                  # Reset window correction parameters\n",
        "                  mags_sum = 0\n",
        "                  resultant_vector = None\n",
        "\n",
        "  return overall_losses"
      ],
      "metadata": {
        "id": "zRiwftD0P8-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_test_accuracy(model, test_dataloader):\n",
        "    # Standard test accuracy calculation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            outputs = model(images.to(device))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.detach().cpu() == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "7wL64_X9P__R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average(data, window_size):\n",
        "    data = np.array(data).astype(float)\n",
        "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')"
      ],
      "metadata": {
        "id": "EI6UbE5zQJ1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_keys_to_str(d):\n",
        "    \"\"\"\n",
        "    Recursively converts all non-string dictionary keys to strings.\n",
        "    \n",
        "    Parameters:\n",
        "    - d (dict): The dictionary to convert.\n",
        "    \n",
        "    Returns:\n",
        "    - dict: A new dictionary with all keys as strings.\n",
        "    \"\"\"\n",
        "    new_dict = {}\n",
        "    for key, value in d.items():\n",
        "        # Convert key to string if it is not already a string\n",
        "        new_key = str(key) if not isinstance(key, str) else key\n",
        "        if isinstance(value, dict):\n",
        "            # Recursively convert nested dictionaries\n",
        "            new_dict[new_key] = convert_keys_to_str(value)\n",
        "        elif isinstance(value, list):\n",
        "            # Convert list elements if they are dictionaries\n",
        "            new_dict[new_key] = [convert_keys_to_str(v) if isinstance(v, dict) else v for v in value]\n",
        "        else:\n",
        "            # Directly copy other values\n",
        "            new_dict[new_key] = value\n",
        "    return new_dict\n",
        "\n",
        "def save_dict_to_file(dictionary, file_path):\n",
        "    \"\"\"\n",
        "    Saves a dictionary to a text file.\n",
        "    \n",
        "    Parameters:\n",
        "    - dictionary (dict): The dictionary to be saved.\n",
        "    - file_path (str): The path to the file where the dictionary will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert all keys to strings\n",
        "        dictionary = convert_keys_to_str(dictionary)\n",
        "        with open(file_path, 'w') as file:\n",
        "            json.dump(dictionary, file, indent=4)\n",
        "        print(f\"Dictionary successfully saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving the dictionary: {e}\")\n"
      ],
      "metadata": {
        "id": "n3BYAXvjQMDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "num_trials = 5\n",
        "\n",
        "train_losses_dict = {}\n",
        "test_accuracies_dict = {}\n",
        "\n",
        "for trial in range(num_trials):\n",
        "  for beta in [0.33, 0.1, 0.05]:\n",
        "    for alpha in [0.1, 0.05, 0.01]:\n",
        "      set_seed(trial)\n",
        "\n",
        "      # Initialize the model\n",
        "      input_dim = 28 * 28\n",
        "      model = LogisticRegression(input_dim)\n",
        "\n",
        "      # Train with ExpTest\n",
        "      train_losses_exptest = train_exptest(model, max_lr, alpha=alpha, beta=beta, epochs=epochs)\n",
        "      test_accuracy_exptest = calculate_test_accuracy(model, test_loader)\n",
        "\n",
        "      if train_losses_dict.get((alpha, beta)) is None:\n",
        "        train_losses_dict[(alpha, beta)] = [train_losses_exptest]\n",
        "      else:\n",
        "        train_losses_dict[(alpha, beta)].append(train_losses_exptest)\n",
        "\n",
        "      if test_accuracies_dict.get((alpha, beta)) is None:\n",
        "        test_accuracies_dict[(alpha, beta)] = [test_accuracy_exptest]\n",
        "      else:\n",
        "        test_accuracies_dict[(alpha, beta)].append(test_accuracy_exptest)\n",
        "\n",
        "      print(f\"Test Accuracy (ExpTest): {test_accuracy_exptest}\")\n",
        "      plt.plot(moving_average(train_losses_exptest, round(len(train_dataset)/batch_size)))\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "bnR85by7QT0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"train_losses_hyperparameters.txt\"\n",
        "save_dict_to_file(train_losses_dict, filename)\n",
        "\n",
        "filename = \"test_losses_hyperparameters.txt\"\n",
        "save_dict_to_file(test_accuracies_dict, filename)\n",
        "for beta in [0.33, 0.1, 0.05]:\n",
        "    for alpha in [0.1, 0.05, 0.01]:\n",
        "        print(\"Mean +/- STD of Test Accuracies ({}, {}):\".format(alpha, beta), round(100*np.mean(test_accuracies_dict[(alpha, beta)]), 2), \"+/-\", round(100*np.std(test_accuracies_dict[(alpha, beta)]), 4))"
      ],
      "metadata": {
        "id": "LjOhaN_MQ3WZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
